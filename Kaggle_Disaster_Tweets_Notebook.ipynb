{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "# \n",
    "This notebook is the final delivery for the lecture \"DataScience SS2020\". \n",
    "This notebook was also used to participate in the following Kaggle Challenge on NLP ([LINK](https://www.kaggle.com/c/nlp-getting-started/overview)). \n",
    "The goal was the interpretation and analysis of the data, preprocessing and the creation of an NLP-ML-model and subsequent prediction. The model was then applied to a submission data set and submitted several times (after applying different models and hyperparameter optimizations).\n",
    "The notebook was created directly in Kaggle and is therefore optimized to run in Kaggle.\n",
    "\n",
    "**My Kaggle ID:** Benedikt Merkel\n",
    "\n",
    "**My best Kaggle Score:** 0,84155\n",
    "\n",
    "\n",
    "# Summary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**This notebook represents two notebooks in one:**\n",
    "\n",
    "**The first part uses the following pre-trained BERT model:**\n",
    "\n",
    "* bert_en_uncased_L-24_H-1024_A-16/2 [LINK](https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/2)\n",
    "* The process is accelerated by the GPU.\n",
    "\n",
    "\n",
    "\n",
    "**The second part uses the following pre-trained BERT model:**\n",
    "\n",
    "* DistilBERT\n",
    "* bert-base-cased\n",
    "* bert-large-cased\n",
    "* bert-large-cased-whole-word-masking-finetuned-squad\n",
    "\n",
    "(You can find the pretrained models here: [LINK](https://huggingface.co/transformers/pretrained_models.html))\n",
    "The process was accelerated with the TCU.\n",
    "\n",
    "\n",
    "# Summary\n",
    "\n",
    "  \n",
    "  \n",
    "**Part 1: bert_en_uncased_L-24_H-1024_A-16/2**\n",
    "*     Imports and check for accelerator unit\n",
    "*     Import the Data \n",
    "*     Data Observation\n",
    "*     Data Cleansing\n",
    "*     Building the Model\n",
    "*     Train the Model\n",
    "*     Results Part 1\n",
    "    \n",
    "    \n",
    "    \n",
    " **Part 2: bert-large-cased, and others**   \n",
    "*     Imports and check for accelerator unit\n",
    "*     Import the Data \n",
    "*     Data Observation\n",
    "*     Data Cleansing\n",
    "*     Building the Model\n",
    "*     Hyperparameters\n",
    "*     Train the Model\n",
    "*     Results Part 2\n",
    "\n",
    "\n",
    " **Overall Conclusion**\n",
    "\n",
    "\n",
    "# My general procedure\n",
    "\n",
    "At first i tried to work with destilBERT as well as bert-base-cased, bert-large-cased, bert-large-cased-whole-word-masking-finetuned-squad and the TCU accelerator, which succeeded. Although the Accuracy, the Validation Accuracy and the Validation loss seemed better than in part 1, i could just achieve  akaggle score of max. 0.8271, even though i achieved better validation accuracies (0,88) and better validation losses (ca. 0,01). While i worked with the different pretrained models, i noticed that my score was approx. 0.01 points better with the large models than with the smaller counterpart.\n",
    "Because of the experiance, i decided to go directly for the large model in part 1 instead of first trying the smaller one, like i did in the 2nd part (which was chronological the first). \n",
    "\n",
    "More details about the scores and parameters can be found under the respective parts.\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "   \n",
    "# Inpirations:\n",
    "\n",
    "\n",
    "* https://www.kaggle.com/raenish/cheatsheet-text-helper-functions\n",
    "* https://www.kaggle.com/datafan07/disaster-tweets-nlp-eda-bert-with-transformers\n",
    "* https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras/data\n",
    "* https://www.kaggle.com/sagar7390/nlp-on-disaster-tweets-eda-glove-bert-using-tfhub*http://education.abcom.com/detecting-slang-using-bert/\n",
    "* https://analyticsindiamag.com/tutorial-on-keras-callbacks-modelcheckpoint-and-earlystopping-in-deep-learning/\n",
    "* https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1 (2nd in my chronological order but first in this notebook because i got the better keggle score with it.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# IMPORTS AND CHECK FOR ACCELERATION UNIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import transformers (no need to install it, it's already included in kaggle)\n",
    "import transformers\n",
    "\n",
    "# original tokenization script created by the Google team\n",
    "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n",
    "import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# import the needed packages\n",
    "\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import urllib\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "#i have activated the GPU acceleration in the notebook, lets see if the GPU can be detected\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT THE DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nlp-getting-started/train.csv\n",
      "/kaggle/input/nlp-getting-started/test.csv\n",
      "/kaggle/input/nlp-getting-started/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# import the datasets directly from kaggle:\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "TRAIN_FILE_PATH = '/kaggle/input/nlp-getting-started/train.csv'\n",
    "TEST_FILE_PATH = '/kaggle/input/nlp-getting-started/test.csv'\n",
    "SUBMISSION_FILE_PATH = '/kaggle/input/nlp-getting-started/sample_submission.csv'\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_FILE_PATH)\n",
    "test_df = pd.read_csv(TEST_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA OBSERVATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        3263 non-null   int64 \n",
      " 1   keyword   3237 non-null   object\n",
      " 2   location  2158 non-null   object\n",
      " 3   text      3263 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 102.1+ KB\n"
     ]
    }
   ],
   "source": [
    "#lets take a first look and see what we got:\n",
    "\n",
    "train_df.info()\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Three people died from the heat wave so far</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Haha South Tampa is getting flooded hah- WAIT ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#raining #flooding #Florida #TampaBay #Tampa 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#Flood in Bago Myanmar #We arrived Bago</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Damage to school bus on 80 in multi car crash ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What's up man?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I love fruits</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Summer is lovely</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My car is so fast</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a goooooooaaaaaal!!!!!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this is ridiculous....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London is cool ;)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love skiing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a wonderful day!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LOOOOOOL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No way...I can't eat that shit</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Was in NYC last week!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love my girlfriend</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cooool :)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Do you like pasta?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword location                                               text  \\\n",
       "0    1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1    4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2    5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3    6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4    7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "5    8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
       "6   10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
       "7   13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
       "8   14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
       "9   15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
       "10  16     NaN      NaN        Three people died from the heat wave so far   \n",
       "11  17     NaN      NaN  Haha South Tampa is getting flooded hah- WAIT ...   \n",
       "12  18     NaN      NaN  #raining #flooding #Florida #TampaBay #Tampa 1...   \n",
       "13  19     NaN      NaN            #Flood in Bago Myanmar #We arrived Bago   \n",
       "14  20     NaN      NaN  Damage to school bus on 80 in multi car crash ...   \n",
       "15  23     NaN      NaN                                     What's up man?   \n",
       "16  24     NaN      NaN                                      I love fruits   \n",
       "17  25     NaN      NaN                                   Summer is lovely   \n",
       "18  26     NaN      NaN                                  My car is so fast   \n",
       "19  28     NaN      NaN                       What a goooooooaaaaaal!!!!!!   \n",
       "20  31     NaN      NaN                             this is ridiculous....   \n",
       "21  32     NaN      NaN                                  London is cool ;)   \n",
       "22  33     NaN      NaN                                        Love skiing   \n",
       "23  34     NaN      NaN                              What a wonderful day!   \n",
       "24  36     NaN      NaN                                           LOOOOOOL   \n",
       "25  37     NaN      NaN                     No way...I can't eat that shit   \n",
       "26  38     NaN      NaN                              Was in NYC last week!   \n",
       "27  39     NaN      NaN                                 Love my girlfriend   \n",
       "28  40     NaN      NaN                                          Cooool :)   \n",
       "29  41     NaN      NaN                                 Do you like pasta?   \n",
       "\n",
       "    target  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "5        1  \n",
       "6        1  \n",
       "7        1  \n",
       "8        1  \n",
       "9        1  \n",
       "10       1  \n",
       "11       1  \n",
       "12       1  \n",
       "13       1  \n",
       "14       1  \n",
       "15       0  \n",
       "16       0  \n",
       "17       0  \n",
       "18       0  \n",
       "19       0  \n",
       "20       0  \n",
       "21       0  \n",
       "22       0  \n",
       "23       0  \n",
       "24       0  \n",
       "25       0  \n",
       "26       0  \n",
       "27       0  \n",
       "28       0  \n",
       "29       0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets take a closer look at the data\n",
    "train_df.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We're shaking...It's an earthquake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>They'd probably still show more life than Arse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hey! How are you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a nice hat?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fuck off!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No I don't like cold!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOOOOOOOOO! Don't do that!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No don't tell me that!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What if?!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Awesome!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>46</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>London</td>\n",
       "      <td>Birmingham Wholesale Market is ablaze BBC News...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>47</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Niall's place | SAF 12 SQUAD |</td>\n",
       "      <td>@sunkxssedharry will you wear shorts for race ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>51</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NIGERIA</td>\n",
       "      <td>#PreviouslyOnDoyinTv: Toke MakinwaÛªs marriag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>58</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Live On Webcam</td>\n",
       "      <td>Check these out: http://t.co/rOI2NSmEJJ http:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>60</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Los Angeles, Califnordia</td>\n",
       "      <td>PSA: IÛªm splitting my personalities.\\n\\n?? t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>69</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>threeonefive.</td>\n",
       "      <td>beware world ablaze sierra leone &amp;amp; guap.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>70</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Washington State</td>\n",
       "      <td>Burning Man Ablaze! by Turban Diva http://t.co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>72</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Whoop Ass, Georgia</td>\n",
       "      <td>Not a diss song. People will take 1 thing and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>75</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>India</td>\n",
       "      <td>Rape victim dies as she sets herself ablaze: A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>84</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SETTING MYSELF ABLAZE http://t.co/6vMe7P5XhC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>87</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>scarborough, ontario</td>\n",
       "      <td>@CTVToronto the bins in front of the field by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>88</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#nowplaying Alfons - Ablaze 2015 on Puls Radio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>90</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>121 N La Salle St, Suite 500</td>\n",
       "      <td>'Burning Rahm': Let's hope City Hall builds a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>94</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Wandering</td>\n",
       "      <td>@PhilippaEilhart @DhuBlath hurt but her eyes a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>99</td>\n",
       "      <td>accident</td>\n",
       "      <td>Homewood, PA</td>\n",
       "      <td>Accident cleared in #PaTurnpike on PATP EB bet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id   keyword                        location  \\\n",
       "0    0       NaN                             NaN   \n",
       "1    2       NaN                             NaN   \n",
       "2    3       NaN                             NaN   \n",
       "3    9       NaN                             NaN   \n",
       "4   11       NaN                             NaN   \n",
       "5   12       NaN                             NaN   \n",
       "6   21       NaN                             NaN   \n",
       "7   22       NaN                             NaN   \n",
       "8   27       NaN                             NaN   \n",
       "9   29       NaN                             NaN   \n",
       "10  30       NaN                             NaN   \n",
       "11  35       NaN                             NaN   \n",
       "12  42       NaN                             NaN   \n",
       "13  43       NaN                             NaN   \n",
       "14  45       NaN                             NaN   \n",
       "15  46    ablaze                          London   \n",
       "16  47    ablaze  Niall's place | SAF 12 SQUAD |   \n",
       "17  51    ablaze                         NIGERIA   \n",
       "18  58    ablaze                  Live On Webcam   \n",
       "19  60    ablaze        Los Angeles, Califnordia   \n",
       "20  69    ablaze                  threeonefive.    \n",
       "21  70    ablaze                Washington State   \n",
       "22  72    ablaze              Whoop Ass, Georgia   \n",
       "23  75    ablaze                           India   \n",
       "24  84    ablaze                             NaN   \n",
       "25  87    ablaze            scarborough, ontario   \n",
       "26  88    ablaze                             NaN   \n",
       "27  90    ablaze    121 N La Salle St, Suite 500   \n",
       "28  94    ablaze                       Wandering   \n",
       "29  99  accident                    Homewood, PA   \n",
       "\n",
       "                                                 text  \n",
       "0                  Just happened a terrible car crash  \n",
       "1   Heard about #earthquake is different cities, s...  \n",
       "2   there is a forest fire at spot pond, geese are...  \n",
       "3            Apocalypse lighting. #Spokane #wildfires  \n",
       "4       Typhoon Soudelor kills 28 in China and Taiwan  \n",
       "5                  We're shaking...It's an earthquake  \n",
       "6   They'd probably still show more life than Arse...  \n",
       "7                                   Hey! How are you?  \n",
       "8                                    What a nice hat?  \n",
       "9                                           Fuck off!  \n",
       "10                              No I don't like cold!  \n",
       "11                         NOOOOOOOOO! Don't do that!  \n",
       "12                             No don't tell me that!  \n",
       "13                                          What if?!  \n",
       "14                                           Awesome!  \n",
       "15  Birmingham Wholesale Market is ablaze BBC News...  \n",
       "16  @sunkxssedharry will you wear shorts for race ...  \n",
       "17  #PreviouslyOnDoyinTv: Toke MakinwaÛªs marriag...  \n",
       "18  Check these out: http://t.co/rOI2NSmEJJ http:/...  \n",
       "19  PSA: IÛªm splitting my personalities.\\n\\n?? t...  \n",
       "20       beware world ablaze sierra leone &amp; guap.  \n",
       "21  Burning Man Ablaze! by Turban Diva http://t.co...  \n",
       "22  Not a diss song. People will take 1 thing and ...  \n",
       "23  Rape victim dies as she sets herself ablaze: A...  \n",
       "24       SETTING MYSELF ABLAZE http://t.co/6vMe7P5XhC  \n",
       "25  @CTVToronto the bins in front of the field by ...  \n",
       "26  #nowplaying Alfons - Ablaze 2015 on Puls Radio...  \n",
       "27  'Burning Rahm': Let's hope City Hall builds a ...  \n",
       "28  @PhilippaEilhart @DhuBlath hurt but her eyes a...  \n",
       "29  Accident cleared in #PaTurnpike on PATP EB bet...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so we got 3 'usable' columns: keyword, locatioin and text (the most important i guess)\n",
    "test_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <td>2533</td>\n",
       "      <td>33.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword</th>\n",
       "      <td>61</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Total  Percentage\n",
       "location   2533       33.27\n",
       "keyword      61        0.80\n",
       "target        0        0.00\n",
       "text          0        0.00\n",
       "id            0        0.00"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#some informations for keywords and locations are missing, lets take a closer look \n",
    "\n",
    "def missing_value_of_data(data):\n",
    "    total=data.isnull().sum().sort_values(ascending=False)\n",
    "    percentage=round(total/data.shape[0]*100,2)\n",
    "    return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])\n",
    "\n",
    "#train data \n",
    "missing_value_of_data(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <td>1105</td>\n",
       "      <td>33.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword</th>\n",
       "      <td>26</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Total  Percentage\n",
       "location   1105       33.86\n",
       "keyword      26        0.80\n",
       "text          0        0.00\n",
       "id            0        0.00"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets also see for the test data\n",
    "missing_value_of_data(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems location data is missing extremly often, if we would drop the missing data, we would loose to much, so we let them be included "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4342</td>\n",
       "      <td>57.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3271</td>\n",
       "      <td>42.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Total  Percentage\n",
       "0   4342       57.03\n",
       "1   3271       42.97"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets count and plot the percentage of the values \n",
    "\n",
    "def count_values_in_column(data,feature):\n",
    "    total=data.loc[:,feature].value_counts(dropna=False)\n",
    "    percentage=round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)\n",
    "    return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])\n",
    "\n",
    "count_values_in_column(train_df,'target')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(y=train_df.target);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can work with that, their distribution should be good enough, interestingly there is more fakes than real reactions\n",
    "\n",
    "# now lets analyse the text itself\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function \n",
    "def create_corpus(target):\n",
    "    corpus=[]\n",
    "    \n",
    "    for x in train_df[train_df['target']==target]['text'].str.split():\n",
    "        for i in x:\n",
    "            corpus.append(i)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'TARGET 1: Punctuations')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "corpus=create_corpus(1)\n",
    "\n",
    "dic=defaultdict(int)\n",
    "import string\n",
    "special = string.punctuation\n",
    "for i in (corpus):\n",
    "    if i in special:\n",
    "        dic[i]+=1\n",
    "        \n",
    "x,y=zip(*dic.items())\n",
    "plt.bar(x,y, color='black')\n",
    "plt.title(\"TARGET 1: Punctuations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'TARGET 0: Punctuations')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "corpus=create_corpus(0)\n",
    "\n",
    "dic=defaultdict(int)\n",
    "import string\n",
    "special = string.punctuation\n",
    "for i in (corpus):\n",
    "    if i in special:\n",
    "        dic[i]+=1\n",
    "        \n",
    "x,y=zip(*dic.items())\n",
    "plt.bar(x,y, color='black')\n",
    "plt.title(\"TARGET 0: Punctuations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are a lot of punctiations in the texts. This is pretty sure caused by twitter mentions, the usage of emoticons etc. ,  so we will delete them in the cleansing process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA CLEANSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# clean text from noise\\ndef clean_text(text):\\n    # filter to allow only alphabets\\n    text = re.sub(r\\'[^a-zA-Z\\']\\', \\' \\', text)\\n    \\n    # remove Unicode characters\\n    text = re.sub(r\\'[^\\x00-\\x7f]+\\', \\'\\', text)\\n    \\n    # convert to lowercase to maintain consistency\\n    text = text.lower()\\n       \\n    return text\\n\\ntrain_df[\\'clean_text\\'] = train_df[\"text\"].apply(clean_text)\\ntest_df[\\'clean_text\\'] = test_df[\"text\"].apply(clean_text)\\n\\ntrain_df.head(20)\\ntest_df.head(20)'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first tried this to clean the data, but there was to much 'junk' left, so cleaned the data even more\n",
    "\n",
    "\n",
    "'''# clean text from noise\n",
    "def clean_text(text):\n",
    "    # filter to allow only alphabets\n",
    "    text = re.sub(r'[^a-zA-Z\\']', ' ', text)\n",
    "    \n",
    "    # remove Unicode characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    \n",
    "    # convert to lowercase to maintain consistency\n",
    "    text = text.lower()\n",
    "       \n",
    "    return text\n",
    "\n",
    "train_df['clean_text'] = train_df[\"text\"].apply(clean_text)\n",
    "test_df['clean_text'] = test_df[\"text\"].apply(clean_text)\n",
    "\n",
    "train_df.head(20)\n",
    "test_df.head(20)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i use a basic helper function to delete the irrelevent information like urls, punctuations, html and symbols, emoticons etc. helperfect by: https://www.kaggle.com/datafan07/disaster-tweets-nlp-eda-bert-with-transformers\n",
    "\n",
    "def clean_tweets(tweet):\n",
    "    \n",
    "    #Removes links and non-ASCII characters\n",
    "    tweet = ''.join([x for x in tweet if x in string.printable])\n",
    "    \n",
    "    # Removing URLs\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    return tweet\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        '['\n",
    "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+',\n",
    "        flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n",
    "    \n",
    "    for p in punctuations:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    text = text.replace('...', ' ... ')\n",
    "    if '...' not in text:\n",
    "        text = text.replace('..', ' ... ')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets apply the functions and remove...\n",
    "\n",
    "#...urls and links \n",
    "train_df[\"text\"] = train_df[\"text\"].apply(lambda x: clean_tweets(x))\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(lambda x: clean_tweets(x))\n",
    "\n",
    "#...emoticons and symbols and related stuff \n",
    "train_df[\"text\"] = train_df[\"text\"].apply(lambda x: remove_emoji(x))\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(lambda x: remove_emoji(x))\n",
    "\n",
    "#...punctuations\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(lambda x: remove_punctuations(x))\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(lambda x: remove_punctuations(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this  # earthquake...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask .  Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to  ' shelter in place '  ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive  # wildfires evacuation ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby  # Alaska a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td># RockyFire Update  =  &gt;  California Hwy .  2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td># flood  # disaster Heavy rain causes flash f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I ' m on top of the hill and I can see a fire ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There ' s an emergency evacuation happening no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I ' m afraid that the tornado is coming to our...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Three people died from the heat wave so far</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Haha South Tampa is getting flooded hah -  WAI...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td># raining  # flooding  # Florida  # TampaBay ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td># Flood in Bago Myanmar  # We arrived Bago</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Damage to school bus on 80 in multi car crash ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What ' s up man ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I love fruits</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Summer is lovely</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My car is so fast</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a goooooooaaaaaal !  !  !  !  !  !</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this is ridiculous .  .  .  .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London is cool  ;  )</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love skiing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a wonderful day !</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LOOOOOOL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No way .  .  . I can ' t eat that shit</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Was in NYC last week !</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love my girlfriend</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cooool  :  )</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Do you like pasta ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword location                                               text  \\\n",
       "0    1     NaN      NaN  Our Deeds are the Reason of this  # earthquake...   \n",
       "1    4     NaN      NaN           Forest fire near La Ronge Sask .  Canada   \n",
       "2    5     NaN      NaN  All residents asked to  ' shelter in place '  ...   \n",
       "3    6     NaN      NaN  13,000 people receive  # wildfires evacuation ...   \n",
       "4    7     NaN      NaN  Just got sent this photo from Ruby  # Alaska a...   \n",
       "5    8     NaN      NaN   # RockyFire Update  =  >  California Hwy .  2...   \n",
       "6   10     NaN      NaN   # flood  # disaster Heavy rain causes flash f...   \n",
       "7   13     NaN      NaN  I ' m on top of the hill and I can see a fire ...   \n",
       "8   14     NaN      NaN  There ' s an emergency evacuation happening no...   \n",
       "9   15     NaN      NaN  I ' m afraid that the tornado is coming to our...   \n",
       "10  16     NaN      NaN        Three people died from the heat wave so far   \n",
       "11  17     NaN      NaN  Haha South Tampa is getting flooded hah -  WAI...   \n",
       "12  18     NaN      NaN   # raining  # flooding  # Florida  # TampaBay ...   \n",
       "13  19     NaN      NaN         # Flood in Bago Myanmar  # We arrived Bago   \n",
       "14  20     NaN      NaN  Damage to school bus on 80 in multi car crash ...   \n",
       "15  23     NaN      NaN                                 What ' s up man ?    \n",
       "16  24     NaN      NaN                                      I love fruits   \n",
       "17  25     NaN      NaN                                   Summer is lovely   \n",
       "18  26     NaN      NaN                                  My car is so fast   \n",
       "19  28     NaN      NaN           What a goooooooaaaaaal !  !  !  !  !  !    \n",
       "20  31     NaN      NaN                     this is ridiculous .  .  .  .    \n",
       "21  32     NaN      NaN                              London is cool  ;  )    \n",
       "22  33     NaN      NaN                                        Love skiing   \n",
       "23  34     NaN      NaN                            What a wonderful day !    \n",
       "24  36     NaN      NaN                                           LOOOOOOL   \n",
       "25  37     NaN      NaN             No way .  .  . I can ' t eat that shit   \n",
       "26  38     NaN      NaN                            Was in NYC last week !    \n",
       "27  39     NaN      NaN                                 Love my girlfriend   \n",
       "28  40     NaN      NaN                                      Cooool  :  )    \n",
       "29  41     NaN      NaN                               Do you like pasta ?    \n",
       "\n",
       "    target  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "5        1  \n",
       "6        1  \n",
       "7        1  \n",
       "8        1  \n",
       "9        1  \n",
       "10       1  \n",
       "11       1  \n",
       "12       1  \n",
       "13       1  \n",
       "14       1  \n",
       "15       0  \n",
       "16       0  \n",
       "17       0  \n",
       "18       0  \n",
       "19       0  \n",
       "20       0  \n",
       "21       0  \n",
       "22       0  \n",
       "23       0  \n",
       "24       0  \n",
       "25       0  \n",
       "26       0  \n",
       "27       0  \n",
       "28       0  \n",
       "29       0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets see how it looks now\n",
    "train_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# there are a lot of repetitive characters in the text, lets get rid of them\\n\\ndef rep(text):\\n    grp = text.group(0)\\n    if len(grp) > 1:\\n        return grp[0:1] # can change the value here on repetition\\ndef unique_char(rep,sentence):\\n    convert = re.sub(r'(\\\\w)\\x01+', rep, sentence) \\n    return convert\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Actually this step caused a drop in the score by 0.04 so i didn't apply it after the first try \n",
    "\n",
    "'''# there are a lot of repetitive characters in the text, lets get rid of them\n",
    "\n",
    "def rep(text):\n",
    "    grp = text.group(0)\n",
    "    if len(grp) > 1:\n",
    "        return grp[0:1] # can change the value here on repetition\n",
    "def unique_char(rep,sentence):\n",
    "    convert = re.sub(r'(\\w)\\1+', rep, sentence) \n",
    "    return convert'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"train_df['text']=train_df['text'].apply(lambda x : unique_char(rep,x))\\ntest_df['text']=test_df['text'].apply(lambda x : unique_char(rep,x))\\ntrain_df.head(30)\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the appliance of the helper function in the step before\n",
    "'''train_df['text']=train_df['text'].apply(lambda x : unique_char(rep,x))\n",
    "test_df['text']=test_df['text'].apply(lambda x : unique_char(rep,x))\n",
    "train_df.head(30)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i used a pretrained bert model: bert_en_uncased_L-24_H-1024_A-16/2; found it in another notebook: https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\n",
    "\n",
    "\n",
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    \n",
    "    if Dropout_num == 0.1:\n",
    "        # Without Dropout\n",
    "        out = Dense(1, activation='sigmoid')(clf_output)\n",
    "    else:\n",
    "        # With Dropout(Dropout_num), Dropout_num > 0\n",
    "        x = Dropout(Dropout_num)(clf_output)\n",
    "        out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(clf_output)\n",
    "    \n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(Adam(lr=6e-6), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.2 s, sys: 1.43 s, total: 12.6 s\n",
      "Wall time: 12.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "##there is also a smaller one \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-768_A-12/2\", but i decided to go directly for the large model\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/2\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = bert_encode(train_df.text.values, tokenizer, max_len=160)\n",
    "test_input = bert_encode(test_df.text.values, tokenizer, max_len=160)\n",
    "train_labels = train_df.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 1024)]       0           keras_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1025        tf_op_layer_strided_slice[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 335,142,914\n",
      "Trainable params: 335,142,913\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(bert_layer, max_len=160)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "381/381 [==============================] - 430s 1s/step - loss: 0.4670 - accuracy: 0.7833 - val_loss: 0.3904 - val_accuracy: 0.8332\n",
      "Epoch 2/8\n",
      "381/381 [==============================] - 389s 1s/step - loss: 0.3542 - accuracy: 0.8504 - val_loss: 0.4174 - val_accuracy: 0.8372\n",
      "Epoch 3/8\n",
      "381/381 [==============================] - 389s 1s/step - loss: 0.2858 - accuracy: 0.8844 - val_loss: 0.4595 - val_accuracy: 0.8437\n",
      "Epoch 4/8\n",
      "381/381 [==============================] - 389s 1s/step - loss: 0.2276 - accuracy: 0.9099 - val_loss: 0.4713 - val_accuracy: 0.8313\n",
      "Epoch 5/8\n",
      "381/381 [==============================] - 389s 1s/step - loss: 0.1810 - accuracy: 0.9274 - val_loss: 0.5456 - val_accuracy: 0.8221\n",
      "Epoch 6/8\n",
      "381/381 [==============================] - 389s 1s/step - loss: 0.1404 - accuracy: 0.9475 - val_loss: 0.6635 - val_accuracy: 0.8148\n",
      "Epoch 7/8\n",
      "381/381 [==============================] - 389s 1s/step - loss: 0.1047 - accuracy: 0.9614 - val_loss: 0.6205 - val_accuracy: 0.8339\n",
      "Epoch 8/8\n",
      "381/381 [==============================] - 389s 1s/step - loss: 0.0888 - accuracy: 0.9701 - val_loss: 0.7071 - val_accuracy: 0.8247\n"
     ]
    }
   ],
   "source": [
    "#the checkpoint callback is used, so it saves the model as a .h5 file, after finishing the training, the model with the lowest validation loss is applied to the test_df.\n",
    "\n",
    "checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "\n",
    "train_history = model.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_split=0.2,\n",
    "    epochs=4,\n",
    "    callbacks=[checkpoint],\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.1 s, sys: 8.53 s, total: 21.6 s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "model.load_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 60s 583ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the submission.csv, extract it, apply the as integer rounded predictions for the test data and save it as a new submission.csv\n",
    "\n",
    "sub = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\n",
    "preds =model.predict(test_input, verbose=1)\n",
    "\n",
    "#round to 0 or 1\n",
    "preds[preds >= 0.5] = 1\n",
    "preds[preds < 0.5] = 0\n",
    "\n",
    "#convert predicted values from float32 to int\n",
    "preds=preds.astype('int') \n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert into the submission table\n",
    "sub['target'] = preds\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS PART 1\n",
    "\n",
    "\n",
    "1. With Cleasning, 8 Epochs, batchzise: 16, validation split: 0.2, DropOut:  0      = kaggle score: 0.84155 \n",
    "2. Without Cleansing, 8 Epochs, batchzise: 16, validation split: 0.2, DropOut:  0      = kaggle score: 0.80135\n",
    "3. With Cleasning, 4 Epochs, batchzise: 16, validation split: 0.2, DropOut:  0      = kaggle score: 0.83849 \n",
    "4. With Cleasning, 4 Epochs, batchzise: 16, validation split: 0.3, DropOut:  0      = kaggle score: 0.83083\n",
    "5. With Cleasning, 16 Epochs, batchzise: 16, validation split: 0.2, DropOut:  0      = kaggle score: 0.83358\n",
    "6. With Cleasning, 8 Epochs, batchzise: 16, validation split: 0.2, DropOut:  0.1      = kaggle score: 0.83450"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2 (1nd in my chronological order but second in this notebook because i got the better keggle score with the other one.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As i mentioned i started using this model, but resigned after 2 days, because i couldn't get a better score than 0.8271. I wanted to include this but anyways because it took a lot of work.\n",
    "To run this bit, you have to anable the TCU in the kaggle notebook, uncommend all the parts und comment out all the parts above.\n",
    "\n",
    "**YOU CAN FIND THE ENDRESULTS FOR THE DIFFERENT PRETRAINED MODELS AT THE END OF THE NOTEBOOKS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS AND CHECK FOR ACCELERATION UNIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# import the needed packages\\n\\n\\nimport numpy as np \\nimport pandas as pd \\npd.set_option('display.max_rows', 100)\\npd.set_option('display.max_colwidth', 1000)\\n\\nimport matplotlib.pyplot as plt\\nfrom tensorflow.keras.layers import Dense, Input\\nfrom tensorflow.keras.optimizers import Adam\\nfrom tensorflow.keras.models import Model\\nimport tensorflow as tf\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nimport transformers\\nfrom tokenizers import BertWordPieceTokenizer\\nimport os\\nimport re\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# import the needed packages\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import transformers\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "import os\n",
    "import re'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Activate the TPU for faster rendering\\n\\nAUTO = tf.data.experimental.AUTOTUNE\\n\\n# Create strategy from tpu\\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\\ntf.config.experimental_connect_to_cluster(tpu)\\ntf.tpu.experimental.initialize_tpu_system(tpu)\\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#Activate the TPU for faster rendering\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Create strategy from tpu\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "strategy = tf.distribute.experimental.TPUStrategy(tpu)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# IMPORT THE DATA \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# import the datasets directly from kaggle:\\n\\nfor dirname, _, filenames in os.walk('/kaggle/input'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\\n\\nTRAIN_FILE_PATH = '/kaggle/input/nlp-getting-started/train.csv'\\nTEST_FILE_PATH = '/kaggle/input/nlp-getting-started/test.csv'\\nSUBMISSION_FILE_PATH = '/kaggle/input/nlp-getting-started/sample_submission.csv'\\n\\ntrain_df = pd.read_csv(TRAIN_FILE_PATH)\\ntest_df = pd.read_csv(TEST_FILE_PATH)\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# import the datasets directly from kaggle:\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "\n",
    "TRAIN_FILE_PATH = '/kaggle/input/nlp-getting-started/train.csv'\n",
    "TEST_FILE_PATH = '/kaggle/input/nlp-getting-started/test.csv'\n",
    "SUBMISSION_FILE_PATH = '/kaggle/input/nlp-getting-started/sample_submission.csv'\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_FILE_PATH)\n",
    "test_df = pd.read_csv(TEST_FILE_PATH)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA OBSERVATION\n",
    "\n",
    "I did the same things here i did in part 1, so i didn't copy them down here, because they aren't essential for running the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA CLEANSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# i use a basic helper function to delete the irrelevent information like urls, punctuations, html and symbols, emoticons etc. helperfect by: https://www.kaggle.com/datafan07/disaster-tweets-nlp-eda-bert-with-transformers\\n\\ndef clean_tweets(tweet):\\n    \\n    #Removes links and non-ASCII characters\\n    tweet = \\'\\'.join([x for x in tweet if x in string.printable])\\n    \\n    # Removing URLs\\n    tweet = re.sub(r\"http\\\\S+\", \"\", tweet)\\n    return tweet\\n\\ndef remove_emoji(text):\\n    emoji_pattern = re.compile(\\n        \\'[\\'\\n        u\\'😀-🙏\\'  # emoticons\\n        u\\'🌀-🗿\\'  # symbols & pictographs\\n        u\\'🚀-\\U0001f6ff\\'  # transport & map symbols\\n        u\\'\\U0001f1e0-🇿\\'  # flags (iOS)\\n        u\\'✂-➰\\'\\n        u\\'Ⓜ-🉑\\'\\n        \\']+\\',\\n        flags=re.UNICODE)\\n    return emoji_pattern.sub(r\\'\\', text)\\n\\n\\ndef remove_punctuations(text):\\n    punctuations = \\'@#!?+&*[]-%.:/();$=><|{}^\\' + \"\\'`\"\\n    \\n    for p in punctuations:\\n        text = text.replace(p, f\\' {p} \\')\\n    text = text.replace(\\'...\\', \\' ... \\')\\n    if \\'...\\' not in text:\\n        text = text.replace(\\'..\\', \\' ... \\')\\n    \\n    return text'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# i use a basic helper function to delete the irrelevent information like urls, punctuations, html and symbols, emoticons etc. helperfect by: https://www.kaggle.com/datafan07/disaster-tweets-nlp-eda-bert-with-transformers\n",
    "\n",
    "def clean_tweets(tweet):\n",
    "    \n",
    "    #Removes links and non-ASCII characters\n",
    "    tweet = ''.join([x for x in tweet if x in string.printable])\n",
    "    \n",
    "    # Removing URLs\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    return tweet\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        '['\n",
    "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+',\n",
    "        flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n",
    "    \n",
    "    for p in punctuations:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    text = text.replace('...', ' ... ')\n",
    "    if '...' not in text:\n",
    "        text = text.replace('..', ' ... ')\n",
    "    \n",
    "    return text'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#lets remove...\\n\\n#...urls and links \\ntrain_df[\"text\"] = train_df[\"text\"].apply(lambda x: clean_tweets(x))\\ntest_df[\"text\"] = test_df[\"text\"].apply(lambda x: clean_tweets(x))\\n\\n#...emoticons and symbols and related stuff \\ntrain_df[\"text\"] = train_df[\"text\"].apply(lambda x: remove_emoji(x))\\ntest_df[\"text\"] = test_df[\"text\"].apply(lambda x: remove_emoji(x))\\n\\n#...punctuations\\ntrain_df[\"text\"] = train_df[\"text\"].apply(lambda x: remove_punctuations(x))\\ntest_df[\"text\"] = test_df[\"text\"].apply(lambda x: remove_punctuations(x))'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#lets remove...\n",
    "\n",
    "#...urls and links \n",
    "train_df[\"text\"] = train_df[\"text\"].apply(lambda x: clean_tweets(x))\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(lambda x: clean_tweets(x))\n",
    "\n",
    "#...emoticons and symbols and related stuff \n",
    "train_df[\"text\"] = train_df[\"text\"].apply(lambda x: remove_emoji(x))\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(lambda x: remove_emoji(x))\n",
    "\n",
    "#...punctuations\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(lambda x: remove_punctuations(x))\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(lambda x: remove_punctuations(x))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# the model i used i got from: http://education.abcom.com/detecting-slang-using-bert/ \\n\\ndef fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\\n    tokenizer.enable_truncation(max_length=maxlen)\\n    tokenizer.enable_padding(max_length=maxlen)\\n    all_ids = []\\n    \\n    for i in tqdm(range(0, len(texts), chunk_size)):\\n        text_chunk = texts[i:i+chunk_size].tolist()\\n        encs = tokenizer.encode_batch(text_chunk)\\n        all_ids.extend([enc.ids for enc in encs])\\n    \\n    return np.array(all_ids)'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# the model i used i got from: http://education.abcom.com/detecting-slang-using-bert/ \n",
    "\n",
    "def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n",
    "    tokenizer.enable_truncation(max_length=maxlen)\n",
    "    tokenizer.enable_padding(max_length=maxlen)\n",
    "    all_ids = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), chunk_size)):\n",
    "        text_chunk = texts[i:i+chunk_size].tolist()\n",
    "        encs = tokenizer.encode_batch(text_chunk)\n",
    "        all_ids.extend([enc.ids for enc in encs])\n",
    "    \n",
    "    return np.array(all_ids)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def build_model(transformer, loss=\\'binary_crossentropy\\', max_len=512):\\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\\n    sequence_output = transformer(input_word_ids)[0]\\n    cls_token = sequence_output[:, 0, :]\\n    x = tf.keras.layers.Dropout(0.35)(cls_token)\\n    out = Dense(1, activation=\\'sigmoid\\')(x)\\n    \\n    model = Model(inputs=input_word_ids, outputs=out)\\n    model.compile(Adam(lr=3e-5), loss=loss, metrics=[tf.keras.metrics.AUC()])\\n    \\n    return model'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def build_model(transformer, loss='binary_crossentropy', max_len=512):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    x = tf.keras.layers.Dropout(0.35)(cls_token)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=input_word_ids, outputs=out)\n",
    "    model.compile(Adam(lr=3e-5), loss=loss, metrics=[tf.keras.metrics.AUC()])\n",
    "    \n",
    "    return model'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# First load the real tokenizer\\ntokenizer = transformers.BertTokenizer.from_pretrained('bert-large-cased')\\n\\n# Save the loaded tokenizer locally\\nsave_path = '/kaggle/working/distilbert_base_uncased/'\\nif not os.path.exists(save_path):\\n    os.makedirs(save_path)\\ntokenizer.save_pretrained(save_path)\\n\\n# Reload it with the huggingface tokenizers library\\nfast_tokenizer = BertWordPieceTokenizer('distilbert_base_uncased/vocab.txt', lowercase=False)\\nfast_tokenizer\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# First load the real tokenizer\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-large-cased')\n",
    "\n",
    "# Save the loaded tokenizer locally\n",
    "save_path = '/kaggle/working/distilbert_base_uncased/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "# Reload it with the huggingface tokenizers library\n",
    "fast_tokenizer = BertWordPieceTokenizer('distilbert_base_uncased/vocab.txt', lowercase=False)\n",
    "fast_tokenizer'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Hyperparameter\\n\\n#all in one place so we can change them faster\\n\\nMAX_LEN = 160 \\nEPOCHS = 8\\nBATCH_SIZE = 16 \\nSTBS=150'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Hyperparameter\n",
    "\n",
    "#all in one place so we can change them faster\n",
    "\n",
    "MAX_LEN = 160 \n",
    "EPOCHS = 8\n",
    "BATCH_SIZE = 16 \n",
    "STBS=150'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#lets split the data set\\n\\ntrain_df, valid_df = train_test_split(train_df, test_size=0.2)'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#lets split the data set\n",
    "\n",
    "train_df, valid_df = train_test_split(train_df, test_size=0.2)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# and apply them to the model\\n\\nx_train = fast_encode(train_df.text.astype(str), fast_tokenizer, maxlen=MAX_LEN)\\nx_valid = fast_encode(valid_df.text.astype(str), fast_tokenizer, maxlen=MAX_LEN)\\nx_test = fast_encode(test_df.text.astype(str), fast_tokenizer, maxlen=MAX_LEN)\\n\\ny_train = train_df.target.values\\ny_valid = valid_df.target.values'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# and apply them to the model\n",
    "\n",
    "x_train = fast_encode(train_df.text.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
    "x_valid = fast_encode(valid_df.text.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
    "x_test = fast_encode(test_df.text.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
    "\n",
    "y_train = train_df.target.values\n",
    "y_valid = valid_df.target.values'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ntrain_dataset = (\\n    tf.data.Dataset\\n    .from_tensor_slices((x_train, y_train))\\n    .repeat()\\n    .shuffle(2048)\\n    .batch(BATCH_SIZE)\\n    .prefetch(AUTO)\\n)\\n\\nvalid_dataset = (\\n    tf.data.Dataset\\n    .from_tensor_slices((x_valid, y_valid))\\n    .batch(BATCH_SIZE)\\n    .cache()\\n    .prefetch(AUTO)\\n)\\n\\ntest_dataset = (\\n    tf.data.Dataset\\n    .from_tensor_slices(x_test)\\n    .batch(BATCH_SIZE)\\n)'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "train_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_train, y_train))\n",
    "    .repeat()\n",
    "    .shuffle(2048)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "valid_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_valid, y_valid))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(x_test)\n",
    "    .batch(BATCH_SIZE)\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# i used a LearningRateScheduler to optimize the time needed for training \\n\\ndef build_lrfn(lr_start=1e-6, lr_max=2e-6, \\n               lr_min=1e-7, lr_rampup_epochs=7, \\n               lr_sustain_epochs=0, lr_exp_decay=.9):\\n    lr_max = lr_max * strategy.num_replicas_in_sync\\n\\n    def lrfn(epoch):\\n        if epoch < lr_rampup_epochs:\\n            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\\n            lr = lr_max\\n        else:\\n            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\\n        return lr\\n    \\n    return lrfn\\n\\n\\n#PLot the funktion\\nplt.figure(figsize=(10, 7))\\n\\n_lrfn = build_lrfn()\\nplt.plot([i for i in range(40)], [_lrfn(i) for i in range(40)]);'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# i used a LearningRateScheduler to optimize the time needed for training \n",
    "\n",
    "def build_lrfn(lr_start=1e-6, lr_max=2e-6, \n",
    "               lr_min=1e-7, lr_rampup_epochs=7, \n",
    "               lr_sustain_epochs=0, lr_exp_decay=.9):\n",
    "    lr_max = lr_max * strategy.num_replicas_in_sync\n",
    "\n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_rampup_epochs:\n",
    "            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n",
    "        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n",
    "            lr = lr_max\n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n",
    "        return lr\n",
    "    \n",
    "    return lrfn\n",
    "\n",
    "\n",
    "#PLot the funktion\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "_lrfn = build_lrfn()\n",
    "plt.plot([i for i in range(40)], [_lrfn(i) for i in range(40)]);'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# The newly introduced focal loss function is created specifically to deal with the data imbalance problem for one-staged detectors. It improves training.\\nfrom tensorflow.keras import backend as K\\n\\ndef focal_loss(gamma=2., alpha=.2):\\n    def focal_loss_fixed(y_true, y_pred):\\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\\n        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\\n    return focal_loss_fixed'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# The newly introduced focal loss function is created specifically to deal with the data imbalance problem for one-staged detectors. It improves training.\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def focal_loss(gamma=2., alpha=.2):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"%%time\\n\\n# here i changed the pretrained models a lot ( )\\n\\nwith strategy.scope():\\n    transformer_layer = (\\n        transformers.TFBertModel.from_pretrained('bert-large-cased')\\n    )\\n    model = build_model(transformer_layer, loss=focal_loss(gamma=1.5), max_len=MAX_LEN)\\nmodel.summary()\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''%%time\n",
    "\n",
    "# here i changed the pretrained models a lot ( )\n",
    "\n",
    "with strategy.scope():\n",
    "    transformer_layer = (\n",
    "        transformers.TFBertModel.from_pretrained('bert-large-cased')\n",
    "    )\n",
    "    model = build_model(transformer_layer, loss=focal_loss(gamma=1.5), max_len=MAX_LEN)\n",
    "model.summary()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#i also used a earlystop callback, but it wasn't that effective and made the kaggle score worse so i commented it out\\n\\n\\n\\n# from keras.callbacks import EarlyStopping\\nlrfn = build_lrfn()\\nlrs = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)\\n# earlystop = EarlyStopping(monitor = 'val_loss',min_delta = 0,patience = 3, verbose = 1,restore_best_weights = True)\\n\\ntrain_history = model.fit(\\n    train_dataset,\\n    steps_per_epoch=STBS,\\n    validation_data=valid_dataset,\\n    epochs=EPOCHS,\\n    batch_size=BATCH_SIZE,\\n    callbacks=[lrs],\\n)\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#i also used a earlystop callback, but it wasn't that effective and made the kaggle score worse so i commented it out\n",
    "\n",
    "\n",
    "\n",
    "# from keras.callbacks import EarlyStopping\n",
    "lrfn = build_lrfn()\n",
    "lrs = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)\n",
    "# earlystop = EarlyStopping(monitor = 'val_loss',min_delta = 0,patience = 3, verbose = 1,restore_best_weights = True)\n",
    "\n",
    "train_history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=STBS,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[lrs],\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#apply model to submission dataset \\nsub = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\\npreds =model.predict(test_dataset, verbose=1)\\n\\n#round to 0 or 1\\npreds[preds >= 0.5] = 1\\npreds[preds < 0.5] = 0\\n\\n#convert predicted values from float32 to int\\npreds=preds.astype('int') \\npreds\\n\\n\""
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#apply model to submission dataset \n",
    "sub = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\n",
    "preds =model.predict(test_dataset, verbose=1)\n",
    "\n",
    "#round to 0 or 1\n",
    "preds[preds >= 0.5] = 1\n",
    "preds[preds < 0.5] = 0\n",
    "\n",
    "#convert predicted values from float32 to int\n",
    "preds=preds.astype('int') \n",
    "preds\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#insert into the submission table\\nsub['target'] = preds\\nsub.to_csv('submission.csv', index=False)\\n\\n\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#insert into the submission table\n",
    "sub['target'] = preds\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS PART 2\n",
    "\n",
    "Strangly the accuracies, val. accuracies and validation losses are a lot better with this models, but somehow the kaggle score stays unter 0.8271.\n",
    "\n",
    "Some examples from the best run with every pretrained model:\n",
    "\n",
    "* destilBERT: 0.75268 (data cleaned; can't remember the hyperparameters)\n",
    "* bert-base-cased: 0.85000 (data cleaned; can't remember the hyperparameters)\n",
    "* bert-large-cased-whole-word-masking-finetuned-squad: 0.82715 (strangly without data cleaning; Batchsize: 64, Steps: 100, Epochs: 4,  max_len: 160)\n",
    "* bert-large-uncased: 0.82562 (with data cleaning; Batchsize: 16, Steps: 150, Epochs: 8, max_len: 160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OVERALL CONCLUSION\n",
    "\n",
    "First of all: the challenge was very interesting. Since I had not worked with NLPs in the labs before, it was a special challenge. Therefore it was helpful to partly use existing solutions like the pre-trained BERT models and to be able to view notebooks of other participants of the challenge. However, I did not get very clever with my results from Part 2. As already mentioned I applied this model first and tested nearly 50 different combinations of models and hyperparameters. Even though the values of the model (validation accuracy, validation loss, and accuracy) were extremely good, the best score I could get was only 0.8271. The only advantage of these models was that they were relatively fast to calculate even with a large number of steps. \n",
    "\n",
    "The bert_en_uncased_L-24_H-1024_A-16/2 model, on the other hand, took much longer (sometimes 20 times longer), but also gave a better Kaggle Score. However, the values (validation accuracy, validation loss, and accuracy ) of the calculated models were a bit worse."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
